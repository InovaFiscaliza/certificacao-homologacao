{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a5ca4b-dcc5-4c8b-a224-40c4a071f9c0",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bf82e5-e79a-4afb-a89d-57c07681d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import configparser\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path as osp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from schwebsearch import load_sch, SCHWebSearch, parse_result_file, save_annotation_file\n",
    "\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import uuid\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from unicodedata import normalize\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9674edfe-3a41-45a6-a78b-c0880c6f4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time format for json result files\n",
    "RESULT_TS_FORMAT = '%Y%m%d%H%M%S%f'\n",
    "# time format for wordcloud files\n",
    "ANNOTATION_TS_FORMAT = '%d/%m/%Y %H:%M:%S'\n",
    "ANOTATION_FILE_TS_FORMAT = '%Y.%m.%d_T%H.%M.%S'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0a3f9-11d9-42b8-a8d6-496c8d3cbb00",
   "metadata": {},
   "source": [
    "# Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e137d375-5b31-409e-8d88-cce09e7e2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args to desenv\n",
    "total_itens_to_query=5\n",
    "grace_period=180\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f6336b-4805-4da2-b6c3-725973ba2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config file ... success.\n"
     ]
    }
   ],
   "source": [
    "# read config file\n",
    "config = configparser.ConfigParser()\n",
    "if osp.exists('websearch_config.ini'):\n",
    "    print('Reading config file ... ', end='')\n",
    "    try:\n",
    "        config.read('websearch_config.ini')\n",
    "        sch_database_file = Path(config['SCHWEBSEARCH']['sch_database_file'])\n",
    "        search_results_folder = Path(config['SCHWEBSEARCH']['search_results_folder'])\n",
    "        parsed_results_folder = Path(config['SCHWEBSEARCH']['parsed_results_folder'])\n",
    "        annotation_folder = Path(config['SCHWEBSEARCH']['annotation_folder'])\n",
    "        print('success.')\n",
    "    except:\n",
    "        print('config file is corrupted. Execution aborted.')\n",
    "        exit(-1)\n",
    "else:\n",
    "    print('Config file not found. Execution aborted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d400c3-c175-425e-8f7a-010d9b67b04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('D:/Datasets/SCHWebSearch/search_results'),\n",
       " WindowsPath('D:/Datasets/SCHWebSearch/parsed_search_results'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results_folder, parsed_results_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c8dd49-f849-4e7c-8098-5463d5fe4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result_file(file, parsed_results_folder=None, max_words=25):\n",
    "\n",
    "    parse_url = lambda url: '.'.join(urlparse(url).netloc.split('.')[-3:])\n",
    "    result_id = str(uuid.uuid4())\n",
    "\n",
    "    EMPTY_RESULT = {'ID': result_id, \n",
    "                    'DataHora': None,\n",
    "                    'Computador': None,\n",
    "                    'Usuário': None, \n",
    "                    'Homologação': None, \n",
    "                    'Atributo': None,\n",
    "                    'Valor': None,  \n",
    "                    'Situação': -1}\n",
    "\n",
    "    if isinstance(file, str):\n",
    "        file = Path(file)\n",
    "    \n",
    "    search_date, search_engine, search_term, _ = re.split('[_.]',file.name)\n",
    "    search_site = None\n",
    "    \n",
    "    try:\n",
    "        with open(file) as f:\n",
    "            results = json.load(f)\n",
    "    except:\n",
    "        return EMPTY_RESULT\n",
    "\n",
    "    try:\n",
    "        with open('blacklisted_sites.txt') as f:\n",
    "            blacklisted_sites = f.readlines()\n",
    "            blacklisted_sites = [site.strip() for site in blacklisted_sites]\n",
    "    except:\n",
    "        blacklisted_sites = None\n",
    "        \n",
    "    lines = []\n",
    "    if search_engine == 'GOOGLE':\n",
    "        # results without items in keys are empty\n",
    "        if 'items' in results.keys():\n",
    "            for item in results['items']:\n",
    "                search_site = item['displayLink']\n",
    "                if is_black_listed_site(search_site):\n",
    "                    continue\n",
    "                else:\n",
    "                    search_site = parse_url(search_site)\n",
    "                    lines.append(item['title'])\n",
    "                    if 'snippet' in item.keys():\n",
    "                        lines.append(item['snippet'])\n",
    "        else:\n",
    "            return EMPTY_RESULT\n",
    "                        \n",
    "    elif search_engine == 'BING':\n",
    "        # results without webPages in keys are empty\n",
    "        if 'webPages' in results.keys():\n",
    "            for item in results['webPages']['value'][:10]:\n",
    "                search_site = item['url']\n",
    "                if is_black_listed_site(search_site):\n",
    "                    continue\n",
    "                else:\n",
    "                    search_site = parse_url(search_site)\n",
    "                    lines.append(item['name'])\n",
    "                    if 'snippet' in item.keys():\n",
    "                        lines.append(item['snippet'])\n",
    "        else:\n",
    "            return EMPTY_RESULT\n",
    "                 \n",
    "    if len(lines) >= 1:\n",
    "        words = tokenizer(' '.join(lines))\n",
    "        words_conter = Counter(words)\n",
    "        wordCloud_dict = {key:value for key,value in words_conter.most_common(max_words)}\n",
    "        wordCloud_json = json.dumps(wordCloud_dict, ensure_ascii=False)\n",
    "        situacao = 1\n",
    "    else:\n",
    "        wordCloud_json = ''\n",
    "        situacao = -1\n",
    "        \n",
    "    wourdCloudInfo_dict = {\n",
    "        'metaData': {\n",
    "            'Version': 1,\n",
    "            'Source': search_engine,\n",
    "            'Mode': 'API',\n",
    "            'Fields': ['Name', 'Snippet'],\n",
    "            'nWords': max_words\n",
    "        },\n",
    "        'searchedWord': search_term,\n",
    "        'cloudOfWords': wordCloud_json\n",
    "        }\n",
    "    \n",
    "    wourdCloudInfo_json = json.dumps(wourdCloudInfo_dict, ensure_ascii=False)\n",
    "    \n",
    "    return {'ID': result_id,\n",
    "            'DataHora': datetime.strptime(search_date,RESULT_TS_FORMAT).strftime(ANNOTATION_TS_FORMAT),\n",
    "            'Computador': os.environ['COMPUTERNAME'],\n",
    "            'Usuário': os.environ['USERNAME'], \n",
    "            'Homologação': f'{search_term[:5]}-{search_term[5:7]}-{search_term[-5:]}', \n",
    "            'Atributo': 'WordCloud',\n",
    "            'Valor': wourdCloudInfo_json,  \n",
    "            'Situação': situacao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa34dfb5-bcfa-45d5-a0f0-81d30e8835ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '54be3f17-ed1c-4006-a559-bdee6cc8cdae',\n",
       " 'DataHora': None,\n",
       " 'Computador': None,\n",
       " 'Usuário': None,\n",
       " 'Homologação': None,\n",
       " 'Atributo': None,\n",
       " 'Valor': None,\n",
       " 'Situação': -1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=r'D:\\Datasets\\SCHWebSearch\\search_results\\20240606090127364095_GOOGLE_057771703345.json'\n",
    "parse_result_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321634f-af04-466c-8b04-61df589de578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
