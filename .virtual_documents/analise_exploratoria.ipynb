





import matplotlib.pyplot as plt
import numpy as np
import os.path as osp
import pandas as pd
import re

from itertools import pairwise
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tqdm.auto import tqdm
from urllib.parse import urlparse
from wordcloud import WordCloud


DEFAULT_BINS = [0,1,2,5,10]
DEFAULT_N_TOP_SITES = 10
NULL_STRING = '#NULO#'


model_stop_words = ['+',  '120hz', '2', '4g', '5g', 'amarelo', 'azul',
                    'basic', 'bluetooth', 'br', 'branco', 'cinza', 'de',
                    'earbuds', 'earphone', 'earphones', 'escuro', 'fone',
                    'gradiente', 'inteligente', 'laranja', 'mi', 'prata',
                    'preto', 'pulseira', 'rosa', 'roxo', 'tela', 'true',
                    'verde', 'wireless', 'wireless']

portuguese_stop_words = stopwords.words('portuguese')
portuguese_stop_words.extend(model_stop_words)
portuguese_stop_words.append('nulo')





def clean_models(models: list):
    pattern = '(([\d]+gb)?[\d]+gb)|(\([\d\w-]+\))|(\d+,\d+)|([+"\/\',’])'
    models = [model.lower() for model in models]
    models = [re.sub(pattern,'',model) for model in models]
    models = list(set([' '.join([token for token in model.split() if token not in model_stop_words]) for model in models]))
    
    return models


def extract_site(url):
    if url:
        parsed_url = urlparse(url).netloc
        for prefix in ['www.', 'empresas.', 'm.', 'produto.', 'lista.', 'sp.']:
            if parsed_url.startswith(prefix):
                return parsed_url[len(prefix):]
        return parsed_url
    else:
        return NULL_STRING


def plot_items_results(df_results,ax=None):

    df = df_results.copy()

    df['results'] = df['url'].apply(lambda url: 0 if url == NULL_STRING else 1)
    df['netloc'] = df['url'].map(extract_site)
    
    results = df[['original_query','results']].groupby('original_query').sum().values.reshape(-1,)
    item_count_no_results = np.where(results==0)[0].shape[0]
    item_count_results = np.where(results>0)[0].shape[0]
    total_results = results.sum()
    
    labels = [f'Itens com resultado:\n{item_count_results:,d}'.replace(',','.'), 
              f'Itens sem resultado:\n{item_count_no_results:,d}'.replace(',','.')]
    values = [item_count_results, item_count_no_results]
    
    startangle = 0
    
    colors = [(0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 0.85),
              (0.8392156862745098, 0.15294117647058825, 0.1568627450980392, 0.85)]
    
    if not ax:
        fig, ax = plt.subplots(figsize=(5, 4), subplot_kw=dict(aspect="equal"))
        plot_show = True
    else:
        plot_show = False
    
    wedges, texts = ax.pie(values, wedgeprops=dict(width=0.5), startangle=startangle, colors=colors)
    
    bbox_props = dict(boxstyle="round,pad=0.3", fc="w", ec="k", lw=0.72)
    kw = dict(arrowprops=dict(arrowstyle="-"),
              bbox=bbox_props, zorder=0, va="center")
    
    kw = dict(arrowprops=dict(arrowstyle="-"),bbox=bbox_props,
              zorder=0, va="center")
    
    if item_count_no_results == 0:
        ang = 135
        y = np.sin(np.deg2rad(ang))
        x = np.cos(np.deg2rad(ang))
        horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
        connectionstyle = "angle,angleA=0,angleB={}".format(ang)
        kw["arrowprops"].update({"connectionstyle": connectionstyle})
        ax.annotate(labels[0], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),
                    horizontalalignment=horizontalalignment, **kw)
    elif item_count_results == 0:
        ang = -45
        y = np.sin(np.deg2rad(ang))
        x = np.cos(np.deg2rad(ang))
        horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
        connectionstyle = "angle,angleA=0,angleB={}".format(ang)
        kw["arrowprops"].update({"connectionstyle": connectionstyle})
        ax.annotate(labels[1], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),
                    horizontalalignment=horizontalalignment, **kw)
    else:
        for i, p in enumerate(wedges):
            
            if item_count_no_results == 0:
                ang = 135
            elif item_count_results == 0:
                ang = -45
            else:
                ang = (p.theta2 - p.theta1)/2. + p.theta1
        
            y = np.sin(np.deg2rad(ang))
            x = np.cos(np.deg2rad(ang))
            horizontalalignment = {-1: "right", 1: "left"}[int(np.sign(x))]
            connectionstyle = "angle,angleA=0,angleB={}".format(ang)
            kw["arrowprops"].update({"connectionstyle": connectionstyle})
            ax.annotate(labels[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),
                        horizontalalignment=horizontalalignment, **kw)
    
    ax.annotate(f'{total_results:,d}\nresultados'.replace(',','.'), (0,0), horizontalalignment='center', verticalalignment='center')
    
    ax.set_title('Quantidade de itens pesquisados e resultados obtidos')
    if plot_show:
        plt.show()


def plot_items_results_count(df_results,bins=DEFAULT_BINS,ax=None):

    df = df_results.copy()

    df['results'] = df['url'].apply(lambda url: 0 if url == NULL_STRING else 1)
    df['netloc'] = df['url'].map(extract_site)
    
    search_results_count = df[['original_query','results']].groupby('original_query').sum().values.reshape(-1,)
    heights,_ = np.histogram(search_results_count,bins=bins)

    x_labels = []
    for start,stop in pairwise(bins):
        if stop==bins[-1]:
            x_labels.append(f'{start} a {stop}')
        elif stop==start+1:
            x_labels.append(f'{start}')
        else:
            x_labels.append(f'{start} a {stop-1}')
    
    bar_colors = ['tab:blue'] * len(heights)
    bar_colors[0] = 'tab:red'
    heights_argsort = heights.argsort()
    if heights_argsort[-1]==0:
        arg_max_count = heights_argsort[-2]
    else: 
        arg_max_count = heights_argsort[-1]
    bar_colors[arg_max_count] = 'tab:orange'
    
    if not ax:
        fig, ax = plt.subplots()
        plot_show = True
    else:
        plot_show = False
    
    for i in range(len(heights)):
        p = ax.bar(x_labels[i],heights[i], color=bar_colors[i], alpha=0.85)
        labels = [f'{heights[i]:,d}'.replace(',','.')]
        if heights[i] > 0:
            ax.bar_label(p, labels=labels, label_type='center')
    
    # remove all the ticks (both axes), and tick labels on the Y axis
    ax.tick_params(bottom=False, left=False,
                   labelbottom=True, labelleft=False)
    
    # remove the frame of the chart
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    
    ax.set_ylabel('Quantidade de itens pesquisados')
    ax.set_xlabel('Resultados obtidos por item')
    ax.set_title('Quantidade de itens pesquisados e resultados obtidos')
    
    if plot_show:
        plt.show()



def plot_items_results_top_sites(df_results,n_top=DEFAULT_N_TOP_SITES,ax=None):

    df = df_results.copy()

    df['results'] = df['url'].apply(lambda url: 0 if url == NULL_STRING else 1)
    df['netloc'] = df['url'].map(extract_site)
    
    df_top_sites = df[df['netloc']!=NULL_STRING]
    df_top_sites = df_top_sites[['netloc','results']].groupby('netloc').sum().sort_values(by='results',ascending=False).head(n_top)
    top_sites = df_top_sites.index.values
    top_sites_count = df_top_sites.results.values
    
    top_sites_percentage = top_sites_count/top_sites_count.sum()
    top_sites_cum_percentage = top_sites_percentage.cumsum()
    bar_colors = ['tab:blue'] * n_top
    top_50_end = np.where(top_sites_cum_percentage>=0.5)[0][1]
    bottom_10 = np.where(top_sites_percentage<0.05)[0]
    for i in range(top_50_end):
        bar_colors[i] = 'tab:green'
    for i in bottom_10:
        bar_colors[i] = 'tab:red'
    
    y_pos = np.arange(len(top_sites))
    
    if not ax:
        fig, ax = plt.subplots()
        plot_show = True
    else:
        plot_show = False
    
    for i in range(n_top):
        p = ax.barh(y_pos[i], top_sites_count[i], color=bar_colors[i], align='center', alpha=0.85)
        labels = [f'{top_sites_count[i]:,d}'.replace(',','.')]
        ax.bar_label(p, labels=labels, label_type='center')
        
    ax.invert_yaxis()  # labels read top-to-bottom
    ax.set_yticks(y_pos, labels=top_sites)
    ax.set_title(f'Top {n_top} sites com mais resultados')
       
    # remove all the ticks (both axes), and tick labels on the Y axis
    ax.tick_params(bottom=False, left=False,
                   labelbottom=False, labelleft=True)
        
    # remove the frame of the chart
    ax.spines['left'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
        
    if plot_show:
        plt.show()


def plot_items_results_dashboard(df_results,bins=DEFAULT_BINS,n_top=DEFAULT_N_TOP_SITES):

    df = df_results.copy()
    
    # Figuras
    fig = plt.figure(figsize=(10, 8), layout='constrained')
    axs = fig.subplot_mosaic([['items_results','items_results_count'],
                              ['items_results_top_sites','items_results_top_sites']])
    for ax in axs:
        fig.get_layout_engine().set(hspace=0.1, wspace=0.1)
    
    
    plot_items_results(df,ax=axs['items_results'])
    plot_items_results_count(df,ax=axs['items_results_count'],bins=bins)
    plot_items_results_top_sites(df,ax=axs['items_results_top_sites'],n_top=n_top)

    axs['items_results'].set_box_aspect(1)
    axs['items_results_count'].set_box_aspect(1)
    
    plt.show()


def plot_top_words(df,n_top_words=50):

    docs_names = df[df['name']!=NULL_STRING]['name'].to_list()
    docs_snippet = df[df['snippet']!=NULL_STRING]['snippet'].to_list()
    
    names_words = []
    snippets_words = []
    
    for name,snippet in zip(docs_names,docs_snippet):    
        name = name.lower()
        snippet = snippet.lower()
        names_words.extend([token for token in word_tokenize(name) if token not in portuguese_stop_words and token.isalpha()])
        snippets_words.extend([token for token in word_tokenize(snippet) if token not in portuguese_stop_words and token.isalpha()])
    
    wc_names = WordCloud(max_words=50,height=300).generate(' '.join(names_words))
    wc_snippets = WordCloud(max_words=50,height=300).generate(' '.join(snippets_words))
    
    fig,ax = plt.subplots(1,2,figsize=(12, 4))
    
    
    ax[0].imshow(wc_names, interpolation="bilinear")
    ax[0].axis("off")
    ax[0].set_title(f'Top {n_top_words} palavras no título')
    
    ax[1].imshow(wc_snippets, interpolation="bilinear")
    ax[1].axis("off")
    ax[1].set_title(f'Top {n_top_words} palavras no resumo')
    
    
    plt.show()








# importar tabela completa do SCH (Dados Abertos)
file_sch = 'datasets/produtos_certificados.zip'
usecols = [0, 1, 2, 3, 11, 12, 13, 14, 15]
dtype = {'Número de Homologação': 'str',
         'CNPJ do Solicitante': 'str'}
df_sch = pd.read_csv(file_sch,sep=';',usecols=usecols,dtype=dtype)

# contar quantidade de números de homologação
df_sch['Quantidade de Número de Homologação']=df_sch[['Número de Homologação','Modelo']].groupby('Número de Homologação').transform('count')
df_sch = df_sch.sort_values(by=['Quantidade de Número de Homologação','Data da Homologação'],ascending=False).reset_index(drop=True)
df_sch.head()


# importar subset do SCH
file_subset_sch = 'datasets/subset_sch.txt'
dtype = {'Número de Homologação': 'str'}
df_subset_sch = pd.read_csv(file_subset_sch,names=['Número de Homologação'],dtype=dtype)

# merge SCH subset and SCH to join columns "Modelo" and "Nome Comercial"
columns_to_merge = ['Número de Homologação', 'Modelo', 'Nome Comercial']
df_subset_sch=df_subset_sch.merge(df_sch[columns_to_merge])
df_subset_sch = df_subset_sch.fillna('#NULO#')
df_subset_sch = df_subset_sch.drop_duplicates()
df_subset_sch.head()


# importa lista de EAN da Xiaomi
file_ean_xiaomi = 'datasets/ean_xiaomi.xlsx'
dtype = {'EAN': 'str'}
df_ean_xiaomi = pd.read_excel(file_ean_xiaomi,dtype=dtype)
df_ean_xiaomi.head()


ean_to_search = df_ean_xiaomi['EAN']
ean_to_search.name='original_query'

ean_models_to_search = pd.Series(clean_models(df_ean_xiaomi['Descrição']))
ean_models_to_search.name='original_query'

sch_to_search = df_subset_sch['Número de Homologação'].drop_duplicates()
sch_to_search.name='original_query'

sch_models_to_search = pd.Series(list(set(df_subset_sch['Modelo'].unique().tolist()+df_subset_sch['Nome Comercial'].unique().tolist())))
sch_models_to_search.name='original_query'


# carregar resultados de pesquisa
file_search_results = 'datasets/search_results/products_search_results.parquet'
df_search_results = pd.read_parquet(file_search_results)

df_search_results = df_search_results.fillna(NULL_STRING)

# dividir resultados de pesquisa em grupos de pesquisa
file_search_results_ean = 'datasets/search_results/products_search_results_ean.parquet'
file_search_results_ean_models = 'datasets/search_results/products_search_results_ean_models.parquet'
file_search_results_sch = 'datasets/search_results/products_search_results_sch.parquet'
file_search_results_sch_models = 'datasets/search_results/products_search_results_sch_models.parquet'

# carregar os arquivos por grupo, se já tiverem sido separados anteriormente,
# caso contrário, separar e salvar o arquivo correspondente
if osp.exists(file_search_results_ean):
    df_search_results_ean = pd.read_parquet(file_search_results_ean)    
    print(f'File {file_search_results_ean} loaded')
else:
    df_search_results_ean = df_search_results.merge(ean_to_search)
    df_search_results_ean.to_parquet(file_search_results_ean)
    print(f'Dataframe df_search_results_sch created and saved to file {file_search_results_ean}')

if osp.exists(file_search_results_ean_models):
    df_search_results_ean_models = pd.read_parquet(file_search_results_ean_models)  
    print(f'File {file_search_results_ean_models} loaded')
else:
    df_search_results_ean_models = df_search_results.merge(ean_models_to_search)
    df_search_results_ean_models.to_parquet(file_search_results_ean_models)
    print(f'Dataframe df_search_results_sch created and saved to file {file_search_results_ean_models}')

if osp.exists(file_search_results_sch):
    df_search_results_sch = pd.read_parquet(file_search_results_sch)   
    print(f'File {file_search_results_sch} loaded')
else:
    df_search_results_sch = df_search_results.merge(sch_to_search)
    df_search_results_sch.to_parquet(file_search_results_sch)
    print(f'Dataframe df_search_results_sch created and saved to file {file_search_results_sch}')

if osp.exists(file_search_results_sch_models):
    df_search_results_sch_models = pd.read_parquet(file_search_results_sch_models)   
    print(f'File {file_search_results_sch_models} loaded')
else:
    df_search_results_sch_models = df_search_results.merge(sch_models_to_search)
    df_search_results_sch_models.to_parquet(file_search_results_sch_models)
    print(f'Dataframe df_search_results_sch created and saved to file {file_search_results_sch_models}')

df_search_results_ean = df_search_results_ean.fillna(NULL_STRING)
df_search_results_ean_models = df_search_results_ean_models.fillna(NULL_STRING)
df_search_results_sch = df_search_results_sch.fillna(NULL_STRING)
df_search_results_sch_models = df_search_results_sch_models.fillna(NULL_STRING)








search_provider = ['Google', 'Bing']
df_ean = {key:df_search_results_ean[df_search_results_ean['search_provider']==key] for key in search_provider}
df_ean_models = {key:df_search_results_ean_models[df_search_results_ean_models['search_provider']==key] for key in search_provider}
df_sch = {key:df_search_results_sch[df_search_results_sch['search_provider']==key] for key in search_provider}
df_sch_models = {key:df_search_results_sch_models[df_search_results_sch_models['search_provider']==key] for key in search_provider}


# ajustar os bins para comportar a quantidade de resultados do Bing
bing_bins = [0,1,2,5,10,50]








plot_items_results_dashboard(df_ean['Google'])
plot_top_words(df_ean['Google'])


plot_items_results_dashboard(df_ean['Bing'],bing_bins)
plot_top_words(df_ean['Bing'])





plot_items_results_dashboard(df_sch['Google'])
plot_top_words(df_sch['Google'])


plot_items_results_dashboard(df_sch['Bing'],bing_bins)
plot_top_words(df_sch['Bing'])





plot_items_results_dashboard(df_ean_models['Google'])
plot_top_words(df_ean_models['Google'])


plot_items_results_dashboard(df_ean_models['Bing'],bing_bins)
plot_top_words(df_ean_models['Bing'])





plot_items_results_dashboard(df_sch_models['Google'])
plot_top_words(df_sch_models['Google'])





query_groups = []

for query_term in ean_to_search.values:
    query_groups.append((query_term,'EAN'))
    
for query_term in ean_models_to_search:
    query_groups.append((query_term,'EAN Model'))

for query_term in sch_to_search:
    query_groups.append((query_term,'SCH'))

for query_term in sch_models_to_search:
    query_groups.append((query_term,'SCH Model'))

df_query_group = pd.DataFrame(query_groups,columns=['original_query','query_group'])

df_check_ean = df_search_results.merge(df_query_group)

df_check_ean['contains_ean'] = df_check_ean.snippet.apply(lambda doc: 1 if 'ean' in doc.lower() else 0)
df_check_ean


df_check_ean.groupby('query_group').agg({'original_query': 'count', 'contains_ean': 'sum'})


df_check_ean[df_check_ean['original_query']=='smartphone poco m4']['url'].to_list()
